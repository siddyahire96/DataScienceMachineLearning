{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION AND MODELLING FOR CHOCOLATE BAR DATASET\n",
    "\n",
    "## Contribution factor\n",
    "\n",
    "\n",
    "### Team Number: Team005 \n",
    "\n",
    "\n",
    "Team Member Name  | Contribution Factor \n",
    "------------- | -------------\n",
    "Karthick Senthil  | 1\n",
    "Siddhesh Ahire   | 1\n",
    "Tarjnee Gandhi  | 1\n",
    "Spandana Reddy   | 1\n",
    "Aakanksha Warankar | 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import json\n",
    "import folium\n",
    "import pickle\n",
    "from branca.colormap import linear\n",
    "from IPython.display import HTML\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data from csv\n",
    "data = pd.read_csv(\"chocolate.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the column names have special characters in them we are changing the name of the column names for easy access\n",
    "\n",
    "new_names = {\n",
    "     data.columns[0]: 'company',\n",
    "    'Specific Bean Origin\\nor Bar Name': 'bar_origin',\n",
    "    'REF': 'review_update_value',\n",
    "    'Review\\nDate': 'review_pub_date',\n",
    "    'Cocoa\\nPercent': 'cocoa_percentage',\n",
    "    'Company\\nLocation': 'company_location',\n",
    "    'Rating': 'rating',\n",
    "    'Bean\\nType': 'bean_type',\n",
    "    'Broad Bean\\nOrigin': 'bean_origin'\n",
    "}\n",
    "data = data.rename(new_names, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White spaces are present in the data and they are treated using the strip() function.  \n",
    "\n",
    "The impossible values present in the data frame are replaced with the NAN values in the attributes  ‘bean_type’ and ‘bean_origin’  and later all the NAN values in the data frame are filled with their respective most repeated value from that attribute. For this simple imputer in imported from sklearn libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stripping the % from the rating column and converting it to a numeric column\n",
    "data['cocoa_percentage'] = (data['cocoa_percentage'].str.strip('%'))\n",
    "data['cocoa_percentage'] = data['cocoa_percentage'].apply(pd.to_numeric)\n",
    "\n",
    "#verifying the converted data\n",
    "data.describe()\n",
    "\n",
    "#Stripping the bean type for spaces\n",
    "data['bean_type'] = data['bean_type'].str.strip()\n",
    "\n",
    "#checking for impossible values\n",
    "data['bean_type'].unique()\n",
    "\n",
    "#replacing the unique value with nan\n",
    "data['bean_type'].replace('\\xc2\\xa0', np.nan,inplace=True)\n",
    "\n",
    "#repeating the same for bean origin\n",
    "data['bean_origin'] = data['bean_origin'].str.strip()\n",
    "data['bean_origin'].replace('\\xc2\\xa0', np.nan, inplace=True)\n",
    "\n",
    "#verifying the values\n",
    "print(data.head())\n",
    "print(data.tail())\n",
    "print(data.describe())\n",
    "print(data.describe(include=['O']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is being done as a classification model the rating column has to be changed into a categorical value. To do this the pandas cut function is used which bins the values in to 5 categories. This is then given a label from 1 to 5 for the model to processes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the grouped values are as above\n",
    "pd.cut(data.rating, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binning the values and assigning labels\n",
    "data['new_rating'] = pd.cut(data.rating, bins = 5, labels = [1,2,3,4,5])\n",
    "data.new_rating = data.new_rating.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fill the missing values simple imputer function from pandas is used. This function fills nan values with the most frequent values from the column. This function is chosen because the columns are categorical, and we didn’t know how to use the fillna function on string type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "pd.DataFrame(imp.fit_transform(data), columns=data.columns,  index=data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Problem Statement:\n",
    "The chocolate dataset has a rating column which is the final column to be predicted by the model. Since this is a numeric column the usual thing to do is go with a regression. But since we need a confusion matrix and accuracy as the output we have decided to go with a classification model. \n",
    "For this rating column is is binned into five different categories and converted into a class label in the previous step.\n",
    "\n",
    "The aim is to build and compare 3 classification models and present the best model based on accuracy that can best predict the rating of chocolate. We will be using the chocolate dataset which contains 1500 observations and 9 attributes.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Learning Algorithms \n",
    "\n",
    "The three learning algorithms selected for the model development are\n",
    "1. RandomForestClassifier\n",
    "2. KNeighborsClassifier\n",
    "3. MLPClassifier\n",
    "\n",
    "The hyperparameter chosen for the tuning is the number of trees in the random forest. We have chosen the values as n= 100, 200, 300, 400, 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Label Encoding and Scaling:\n",
    " \n",
    "Encoding of Categorical or Non-Numeric features: As all Categorical Features are non-numeric, encoding has been done using Label Encoder. \n",
    "In the above code, we have used the LabelEncoder class from sklearn preprocessing to transform the labels for 'company','bar_origin','company_location','bean_type', 'bean_origin' , where alphabetically each category is assigned numbers starting from 0.This leads us to another challenge when working with the machine learning models. Since all mathematical models deal with numbers and some numbers are greater than others, this can skew the models leading to inaccurate results. \n",
    "\n",
    "\n",
    "\n",
    "We Applied Z-score normalisation (i.e., standardisation) to re-scale each feature of the chocolate data such that each of the re-scaled features has zero mean and unit standard deviation by using the sklearn.preprocessing.StandardScaler2 method. We have many independent variables but the scale of each value has a high range, for example :Bar Origin and Company.that so high and that would be a problem for our ML model, so we need to scale all of our variables with StandardScaler : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the X and y variables from the data\n",
    "to_drop = [\"rating\",\"new_rating\"]\n",
    "X = data.drop(to_drop, axis=1).copy()\n",
    "y = data.new_rating\n",
    "\n",
    "#using the label encoder to convert the categorical values to numeric values\n",
    "categorical = ['company','bar_origin','company_location','bean_type', 'bean_origin']\n",
    "for feature in categorical:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X[feature] = le.fit_transform(X[feature])\n",
    "#verifying the converted data\n",
    "print(X.head())\n",
    "print(X.tail())\n",
    "#scaling the data using the standard scaler\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\n",
    "print(X.head())\n",
    "print(X.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Partioning:\n",
    "\n",
    "For modeling purpose all the categorical data is then converted into the numerical data type and then split into train and test dataset so that we can train the model on the training dataset and then predict the rating of the chocolate. All the features are stored into X variable and then encoded into numerical values whereas the Y variable has the label stored I.e. Rating attribute. \n",
    "\n",
    "The splitting is done with the test size of 0.3 I.e. in the ratio of 70:30. \n",
    "The data is divided sequentially into first 30percent of data into test data and the later 70 percent of the data into train dataset.\n",
    "\n",
    "The training dataset is then scaled using the standard scaler function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'x_train:', x_train\n",
    "print 'x_test:', x_test\n",
    "print 'y_train:',y_test\n",
    "print 'y_train:',y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. K Fold Cross Validation:\n",
    "\n",
    "All the three models employed are then compared and evaluated. The method used for comparison is known as K Fold cross validation statistical method. The value of the k is set as 5 with random state 0. The five folds of the dataset is being printed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining kfold validation with n=5\n",
    "kfold = KFold(n_splits = 5,random_state=0)\n",
    "for train_index, test_index in kfold.split(x_train):\n",
    "    print 'train_index:', train_index\n",
    "    print 'test_index:', test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a models array and adding the models in to the array\n",
    "models = []\n",
    "models.append(('KNN',KNeighborsClassifier()))\n",
    "models.append(('RF - n=100', RandomForestClassifier(n_estimators=100)))\n",
    "models.append(('RF - n=200', RandomForestClassifier(n_estimators=200)))\n",
    "models.append(('RF - n=300', RandomForestClassifier(n_estimators=300)))\n",
    "models.append(('RF - n=400', RandomForestClassifier(n_estimators=400)))\n",
    "models.append(('RF - n=500', RandomForestClassifier(n_estimators=500)))\n",
    "models.append(('MLP', MLPClassifier()))\n",
    "results = []\n",
    "modelnames = []\n",
    "scoring = 'accuracy'\n",
    "for modelname, model in models:\n",
    "    \n",
    "    model_result = cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(model_result)\n",
    "    modelnames.append(modelname)\n",
    "    print(\">%s: %f (%f)\" % (modelname, model_result.mean(), model_result.std()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above comparison results we can see that the random forest has the most highest accuracy among the three algorithms. And at the time of writing this the n=300 gave the best output accuracy among the random forest hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Testing the prediction model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=300)\n",
    "rf_train = rf.fit(x_train,y_train)\n",
    "y_pred = rf_train.predict(x_test)\n",
    "#calculate the confusion matrix\n",
    "cm = (confusion_matrix(y_test, y_pred))\n",
    "\n",
    "#calculate the accuracy\n",
    "acc = (100*accuracy_score(y_test, y_pred))\n",
    "\n",
    "#calculate the classification report\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "#print the output\n",
    "print(cm)\n",
    "print(\"Accuracy: %s%%\" % acc)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the predicted data to the test data as new column\n",
    "y_test = y_test.to_frame()\n",
    "dataset = pd.DataFrame.from_records(y_pred)\n",
    "y_test[\"predicted\"] = dataset[0].values\n",
    "print(y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting model to pickle\n",
    "f = open('model.pickle','wb')\n",
    "pickle.dump(rf_train, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Analysis\n",
    "\n",
    "## 7.1 Feature importance\n",
    "In order to understand the prediction and which features are important, we can get the feature importances from the predicted model. This can be done by taking the feature importances from the trained model. The top 10 important features are listed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the indices that would sort the array \n",
    "sorted_indices = np.argsort(rf_train.feature_importances_)\n",
    "\n",
    "#finding the most important features and associated importance\n",
    "variables = rf_train.feature_importances_[sorted_indices]\n",
    "importance_rating = x_train.columns.values[sorted_indices]\n",
    "\n",
    "importances = pd.DataFrame({'variable':variables, 'importance':importance_rating})\n",
    "importances.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Map out chocolate rating geographically on the map\n",
    "To map locaion geographically, we are using Folium python library to visualize data and create an interective map in HTML file. Also we are using GeoJson since We can plot them all at once without a loop.\n",
    "To explore the rating of the bean geographically, We have choosed to group bean origin and map avarage rating using colour by different colors. \n",
    " - How the rating of chocolate varies as per the bean origin location?\n",
    " - Map rating geographically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average rating of bean_origin\n",
    "bean_origin_ratings = data.groupby(\"bean_origin\").rating.mean()\n",
    "\n",
    "# load world contries location json\n",
    "geo_json_data = json.load(open(\"world-countries.json\"))\n",
    "\n",
    "# Display colorscale based on rating values\n",
    "colormap = linear.YlGnBu_09.scale(\n",
    "    bean_origin_ratings.min(),\n",
    "    bean_origin_ratings.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a function to color bean origin location by their rating scale and generate map using GeoJson. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to map rating on color scale\n",
    "used_names = []\n",
    "def make_color(feature):\n",
    "    current_country = feature['properties']['name']\n",
    "    if current_country in bean_origin_ratings.index.values:\n",
    "        used_names.append(current_country)\n",
    "        return colormap(bean_origin_ratings[feature['properties']['name']])\n",
    "    else:\n",
    "        return \"floralwhite\"\n",
    "    \n",
    "#  Map bean orgin with ratings\n",
    "m = folium.Map([0,0], zoom_start=1.5)\n",
    "\n",
    "folium.GeoJson(\n",
    "    geo_json_data,\n",
    "    name='mean ratings of bean origin',\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': make_color(feature),\n",
    "        'color': 'grey',\n",
    "        'weight': 1,\n",
    "        'dashArray': '3, 3',\n",
    "        'fillOpacity': 0.8,\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "colormap.caption = 'mean ratings of bean origin'\n",
    "colormap.add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m.save(\"bean_origin_ratings.html\")\n",
    "HTML('<iframe src=bean_origin_ratings.html width=800 height=450></iframe>')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking to map we can identify\n",
    "- The beans which has origin from Brazil, Vietnam and Guatemala used in chocolate has higher average rating.\n",
    "- Majority of the bean origin are from the South America with higher average ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Identify correlation between the feature and How much percentage of cocoa is good for chocolate?\n",
    "\n",
    "It is interesting to find relation between the features as it will help to predict the values of one of these features given the other features. But we can not be sure of it until we have conclusive evidence regarding the same. To identify relation we are using correlation matrix and heatmap plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-relation between the features\n",
    "print(data.corr())\n",
    "sns.heatmap(data.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KEY OBSERVATION\n",
    "- Review update date and review update score are highly correlated.\n",
    "- It is interesting to know that cocoa percentage and ratings are negatively correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring relation between the cocoa percentage and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (a,b) = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "sns.distplot(data.cocoa_percentage, kde=False, ax=a)\n",
    "a.set_xlabel(\"% of cocoa\", size=12,color='darkred')\n",
    "a.set_ylabel(\"frequency\", size=12,color='darkred')\n",
    "a.set_title(\"Distribution of cocoa percentage\");\n",
    "\n",
    "sns.boxplot(x=data.new_rating, y=data.cocoa_percentage, ax=b);\n",
    "b.set_xlabel(\"New Ratings\", size=12,color='darkred')\n",
    "b.set_ylabel(\"% of cocoa\", size=12,color='darkred')\n",
    "b.set_title(\"Comparision of rating with cocoa percentage\");\n",
    "\n",
    "# We will use Seaborn to map\n",
    "sns.lmplot(x='cocoa_percentage',y='new_rating',fit_reg=False,scatter_kws={\"color\":\"darkred\",\"alpha\":0.3,\"s\":100}\n",
    "           ,data=data) #syntax for scatter plot\n",
    "\n",
    "plt.xlabel('Percentage of Cocoa',size=12,color='darkred')\n",
    "plt.ylabel('Expert Rating of the Bar',size=12,color='darkred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plots above, the relation between the rating and cocoa percentage is quite ambiguous. But, we can notice that majority of the chocolate has cocoa percentage between 70 to 80. When comparing percentage of cocoa with rating it is clearly shows that most chocolate bar ratings are between 2 to 4 and few of them are with the 5 ratings. Interesting fact is that cocoa percentage less than 50% and higer than 90% have rating less than 3 i.e very high percentage of cocoa have a tendency to be not delicious. This might be reason of bitter compounds in cocoa. Also cocoa percentage is not a strong factor to predict chocolate rating as correlation and plots does not imply a well define correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
